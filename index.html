<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<title>cgdsss</title>
<link rel="shortcut icon" href="favicon.png"/>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link rel="stylesheet" type="text/css" href="main.css" id="css">
<!-- <style type="text/css">
</style> -->
<link rel="shortcut icon" href="./pic/ustc.ico">
<!-- <script>
  var request = new XMLHttpRequest();
  
  request.open('GET', 'https://api.ipdata.co/?api-key=e70fe0136eacb22381b6d701d5dc0425ada89dff7d9a9754db77042b');
  
  request.setRequestHeader('Accept', 'application/json');
  
  request.onreadystatechange = function () {
    if (this.readyState === 4) {
      if (this.responseText.search("China") != -1){
        window.location.href="https://cgdsss.gitee.io/cgd/";
      }
    }
  };
  
  request.send();
</script> -->
<script>
  function validataOS(){
if(navigator.userAgent.indexOf("Window")>0){
return "Windows";
}else if(navigator.userAgent.indexOf("Mac OS X")>0) {
return "Mac";
}else if(navigator.userAgent.indexOf("Linux")>0) {
return "Linux";
}else{
return "NUll";
}
}
if (validataOS() == "Linux") {
  var obj = document.getElementById("css");
  obj.setAttribute("href","mainlinux.css");
}
if (validataOS() == "Mac") {
  var obj = document.getElementById("css");
  obj.setAttribute("href","mainlinux.css");
}
</script>
</head>
<body>
  <button onclick="topFunction()" id="myBtn" title="top"></button>
  <div align="center" class="backgrounddiv">
  <div id = "page_size" style="width:1000px" >
  <div class="divpad fontgrounddiv shadow">
  <div>
    <div class="maxwidth leftfloat" align="left">
      <p>
        <br>
        <a class="blacklink" style="font-size: 2em;" href="https://cgdsss.github.io/"><b>Guangda Chen</b></a>
        <a itemprop="sameAs" content="https://orcid.org/0000-0003-1888-9947" href="https://orcid.org/0000-0003-1888-9947" target="orcid.widget" rel="noopener noreferrer" style="vertical-align:top;"><img src="https://orcid.org/sites/default/files/images/orcid_16x16.png" style="width:1em;margin-right:.5em;" alt="ORCID iD icon"></a>
        <!-- <br> -->
        <!-- <div id = "title_br"><br></div> -->
      </p>
    </div>
  </div>
  <div id = "title_div">
    <div class="width70 leftfloat" align="left" id = "title_div_left">
      <p>
        <br>
        <a><b>Address : </b></a>
        <br>
        NetEase, 399 Wangshang Road, Binjiang District, Hangzhou, China.<a href="https://surl.amap.com/UVunOpU8D8" id="icon" ><img src="./pic/gmap.ico" /></a>
        <br><br>
        <a><b>Email: </b></a>
        <br>
        <a href=mailto:cgdsss@mail.ustc.edu.cn class="bluelink" ">cgdsss@mail.ustc.edu.cn</a>; 
        <a href=mailto:chenguangda@corp.netease.com class="bluelink" ">chenguangda@corp.netease.com</a>
        <br><br>
        <a style="line-height:1.8;"><b>Navigator: </b></a>
        <br>
        <a href="#about" class="fontlarge bluelink bb">About</a> 
        <a href="#employ" class="fontlarge bluelink bb">Employment</a> 
        <a href="#edu" class="fontlarge bluelink bb">Education</a>  
        
        <a href="#project" class="fontlarge bluelink bb">Project</a>
        <a href="#pubs" class="fontlarge bluelink bb">Publication</a>
      </p>
    </div>
    <div class="width30 leftfloat" align="center" id = "title_div_right">
      <a href="./day.html" id="icon" ><img id="imglove" style="width: 100%" src="./pic/CGD2.jpg" /></a>
      <!-- <a href="./day.html" id="icon" ><img id="imglove" style="width: 100%" src="./pic/b.jpg" /></a> -->
      <!-- <a id="wordlove" style="font-family: Comic Sans MS; font-size: 1.1em;"> <I>My sally girl, My love</I></a> -->
    <div id="img_br">
      <br>
      <br>
    </div>
    </div>
    <hr color=#987cb9 SIZE=2 />
  </div>
  
  <div id="about">
    <div class="width25 leftfloat" align="left" id="about_title">
      <br>
      <a class="fontbig blacklink">About</a>
    </div>
    <div class="width75 leftfloat linebigheight" align="left" id="about_content">
      <p style="text-align:justify;">
        Dr. Guangda Chen is currently working as a Senior Robot Engineer at <a href="https://fuxi.163.com/"  class="bluelink">Fuxi Robotics in NetEase</a>.
        And he is also a postdoctoral researcher at the College of Control Science and Engineering, <a href="https://www.zju.edu.cn/"  class="bluelink">Zhejiang University</a>
        (advised by Prof. <a href="https://person.zju.edu.cn/0097062"  class="bluelink">Rong Xiong</a>).
        He received his Ph.D. degree in Computer Science from 
      <a href="https://www.ustc.edu.cn/" class="bluelink" >University of Science and Technology of China </a> in 2021 
      (BA17011, advised by Prof. 
      <a href="https://cs.ustc.edu.cn/2020/0828/c23235a460075/pagem.htm"  class="bluelink">Xiaoping Chen</a> in the 
      <a href="http://ai.ustc.edu.cn/" class="bluelink" >USTC Robotics Laboratory</a>).
      Before joining USTC Robotics Lab in 2015, 
      he received a Bachelor of Administration from <a href="http://www.cmu.edu.cn/"  class="bluelink">China Medical 
        University</a> in 2014. 
        As a graduate student, he focused on researching mobile robot navigation in dynamic and crowded environments. 
        Currently, he is dedicated to developing and applying innovative automation and intelligent technologies in heavy construction machinery.
      <br><br>
      Find him on:
      <br>
      <a href="https://github.com/cgdsss"   id="icon" title="GitHub"><img src="./pic/github.ico" /></a>
      <a href="https://scholar.google.com/citations?user=h7vRddgAAAAJ&hl=zh-CN" id="icon"  title="Google Scholar"><img src="./pic/gs.ico" /></a>
      <a href="https://space.bilibili.com/14005427"   id="icon" title="bilibili"><img src="./pic/bili.ico" /></a>
      <a href="https://www.douban.com/people/cgdsss/"   title="豆瓣" id="icon"><img src="./pic/douban.ico" /></a>
      <a href="http://weibo.com/cgdsss"   title="新浪微博" id="icon"><img src="./pic/weibo.ico" /></a>
      <a href="https://music.163.com/#/user/home?id=430012241"   title="网易云音乐" id="icon"><img src="./pic/music.ico" /></a>
      <a href="https://www.linkedin.com/in/%E5%B9%BF%E5%A4%A7-%E9%99%88-1b0770109/"   id="icon" title="Linkedin"><img src="./pic/link.ico" /></a>
      <a href="https://www.facebook.com/guangda.chen.9"   title="facebook" id="icon"><img src="./pic/facebook.ico" /></a>
      <a href="https://twitter.com/cgdsss_USTC"   title="Twitter" id="icon"><img src="./pic/twitter.ico" /></a>
      <a href="https://www.instagram.com/guangdachen/"   title="instagram" id="icon"><img src="./pic/ins.ico" /></a>
      <a href="https://plus.google.com/u/0/100646927760929911525"   title="Google+" id="icon"><img src="./pic/googleplus.ico" /></a>
      </p>
    </div>
    <hr color=#987cb9 SIZE=1 />
  </div>
  
  <div id="employ">
    <div class="width25 leftfloat" align="left" id="employ_title">
      <br>
      <a class="fontbig blacklink">Employment</a>
    </div>
    <div class="width75 leftfloat linebigheight" align="left" id="employ_content">
      <p>
      <a href="https://ir.netease.com/"  class="fontnorm bluelink"> NetEase, Inc.</a>, 
      <a class="fontsmall">Senior Robot Engineer</a>
      <br>
      - <a href="https://fuxi.163.com/"  class="bluelink"> Fuxi Robotics</a>, 
      <a class="fontsmall"><em>2021 — now</em></a>
      <br>
      </p>
      <p>
      <a href="https://www.zju.edu.cn/"  class="fontnorm bluelink"> Zhejiang University</a>, 
      <a class="fontsmall">Postdoc [Prof. <a href="https://person.zju.edu.cn/0097062">Rong Xiong</a>]</a>
      <br>
      - <a href="http://www.cse.zju.edu.cn/"  class="bluelink"> College of Control Science and Engineering</a>, 
      <a class="fontsmall"><em>2023 — now</em></a>
      <br>
      </p>
    </div>
    <hr color=#987cb9 SIZE=1 />
  </div>

  <div id="edu">
    <div class="width25 leftfloat" align="left" id="edu_title">
      <br>
      <a class="fontbig blacklink">Education</a>
    </div>
    <div class="width75 leftfloat linebigheight" align="left" id="edu_content">
      <p>
      <a href="https://www.ustc.edu.cn/"  class="fontnorm bluelink"> University of Science and Technology of China</a>, 
      <a class="fontsmall">PhD [Prof. 
        <a href="https://cs.ustc.edu.cn/2020/0828/c23235a460075/pagem.htm" >Xiaoping Chen</a>]</a>
      <br>
      - <a href="http://cs.ustc.edu.cn/"  class="bluelink"> School of Computer Science and Technology</a>, 
      <a class="fontsmall"><em>2015 — 2021</em></a>
      <br>
      </p>
      <p>
      <a href="http://www.cmu.edu.cn/"  class="fontnorm bluelink"> China Medical University</a>, 
      <a class="fontsmall">BAdmin</a>
      <br>
      - <a href="https://www.cmu.edu.cn/cmudmi/xygk/xyjj.htm"  class="bluelink"> Medical Informatics</a>, 
      <a class="fontsmall"><em>2011 — 2014</em></a>
      <br>
      - <a href="http://school.cmudental.com/"  class="bluelink"> Stomatology</a>, 
      <a class="fontsmall"><em>2010 — 2011</em></a>
      <br>
      </p>
    </div>
    <hr color=#987cb9 SIZE=1 />
  </div>
  
  <div id="project">
    <div class="width25 leftfloat" align="left" id="project_title">
      <br>
      <a class="fontbig blacklink">Project</a>
    </div>
    <div class="width75 leftfloat linebigheight" align="left" id="project_content">
      <p>
        <a  class="fontnorm bluelink pub-venue" href="https://fuxi.163.com/productSummary/wj"> <b>Heavy Machine Automation
          </b></a> <a class="fontverysmall"><em>(2021 - now)</em></a> <br>
          Planning and Control Group, Fuxi Robotics in NetEase.
        <ul>
          <li>
            <a href="https://fuxi.163.com/productSummary/wj"  class="bluelink">Robotic excavator</a> 
            <a href="pdf/Modeling and Control of General Hydraulic Excavator for Human-in-the-loop Automation.pdf"  class="bluelink">[PDF]</a>, <a href="https://youtu.be/N6I0WZGSF68"  class="bluelink">[Demo]</a>
          </li>
          <li>
            <a href="https://fuxi.163.com/productSummary/loader"  class="bluelink">Unmanned loader</a>
            <a href="pdf/2023中国互联网大会创新十大典型案例.jpg"  class="bluelink">[Media]</a>
          </li>
        </ul>
      </p>
      <hr style="height:1px;border:none;border-top:1px dashed #0066CC;"/>
      <p>
        <a  class="fontnorm bluelink pub-venue" href="pic/PandaBot.jpg"> <b>Panda Robot</b></a> <a class="fontverysmall"><em>(2020 - 2021)</em></a>  <br> 
        Tour guide robot at the <a href="http://www.panda.org.cn/china/events/museum/2021-03-16/8573.html" class="bluelink">Chengdu Giant Panda Museum. </a> <br>
        <a class="pub-authors">Group Leader, </a> (<a href="pdf/phd_cgd.pdf#page=112" class="bluelink">PhD thesis</a>, <a href="pdf/phd_cgd.pdf#page=90" class="bluelink">Ch. 4</a> and <a href="pdf/phd_cgd.pdf#page=112" class="bluelink">Ch. 5</a>)<br>
        <ul>
          <li>
            Pedestrian detection and tracking, <a href="https://www.bilibili.com/video/BV1HB4y1P7xE"  class="bluelink">[Demo]</a>
          </li>
          <li>
            Long-term robust localization, <a href="https://www.bilibili.com/video/BV1q5411w7qX"  class="bluelink">[Demo]</a>
          </li>
          <li>
            Navigation in dense crowds, <a href="https://www.bilibili.com/video/BV13Z4y1A7br"  class="bluelink">[Brief Demo]</a>, <a href="https://www.bilibili.com/video/BV1wU4y1V7cA?share_source=copy_web"  class="bluelink">[Full Demo]</a>
          </li>
          <li>
            <a style="color: #0066CC;"> Tour guide service v1.0</a>, <a href="https://www.bilibili.com/video/BV1UM4y1u7t8"  class="bluelink">[Demo]</a>
          </li>
        </ul>
      </p>
      <hr style="height:1px;border:none;border-top:1px dashed #0066CC;"/>
      <p>
        <a href="https://github.com/DRL-Navigation" class="fontnorm bluelink pub-venue"> <b>DRL-based Navigation</b></a> <a class="fontverysmall"><em>(2019 - now)</em></a> <br> 
        <a class="pub-authors">Group Leader </a>, <a href="https://github.com/DRL-Navigation" class="award ">Open source code on GitHub</a>
        <br>
        <ul>
          <li>
            <a href="pdf/ICNSC_2020_paper_11.pdf" class="bluelink"> DQN-based Obstacle Avoidance</a>, <a href="https://youtu.be/Eq4AjsFH_cU"  class="bluelink">[Demo]</a>
          </li>
          <li>
            <a href="pdf/IROS_2021_DRQNNav.pdf" class="bluelink"> DRQN-based 3D Obstacle Avoidance</a>, <a href="https://youtu.be/cnCid0qOZg4"  class="bluelink">[Demo]</a>
          </li>
          <li>
            <a href="pdf/ICTAI_2020.pdf" class="bluelink"> Multi-Robot Collision Avoidance</a>, <a href="https://youtu.be/KOb1q23L7-U"  class="bluelink">[Demo]</a>
          </li>
          <li>
            <a href="pdf/IROS_2021_PedNav.pdf" class="bluelink">Crowd Navigation</a>, <a href="https://www.bilibili.com/video/BV1Vb4y1D7R6"  class="bluelink">[Demo]</a>, <a href="gitstats/authors.html" class="bluelink" target="_blank"> [Code statistics] </a>
          </li>
          <li>
            <a href="pdf/ICRA2022_BEEP_FINAL_06_03.pdf" class="bluelink">Crowd Navigation with Interaction Capacity</a>
          </li>
        </ul>
      </p>
      <hr style="height:1px;border:none;border-top:1px dashed #0066CC;"/>
      <p>
      <a href="https://ai.ustc.edu.cn/en/robocup/atHome/media.php"  class="fontnorm bluelink"> <b>Kejia Robot</b></a> <a class="fontverysmall"><em>(2015 - 2019)</em></a>
      <br>
      Team: <a href="http://ai.ustc.edu.cn/en/robocup/atHome"  class="fontnorm bluelink">WrightEagle@Home</a>
      <br>
      <a href="pdf/tdp19.pdf"  class="bluelink">[TDP <img src="./pic/pdf.gif" />]</a>, <a href="pdf/poster19_athome.pdf"  class="bluelink">[Poster <img src="./pic/pdf.gif" />]</a>, <a href="pdf/ECRDC_USTC.pdf"  class="bluelink">[Report <img src="./pic/pdf.gif" />]</a>, <a href="https://youtu.be/sWi9EOKIhlE"  class="bluelink">[Demo]</a>
      <table border="1" cellpadding="3" cellspacing="0">
        <tr>
          <th>Event</th>
          <th>Place</th>
          <th>Award</th>
          <th>Role</th>
        </tr>
        <tr>
          <td><a href="http://www.rcccaa.org/zdy/gui.html"  class="bluelink"> RoboCup China Open@Home league 2015 </a></td>
          <td><a class="pub-authors fontsmall"> Guiyang, China </a></td>
          <td><a class="pub-authors fontsmall">Champion</a></td>
          <td><a class="pub-authors fontsmall">Major</a></td>
        </tr>
        <tr>
          <td><a href="https://www.robocup.org/events/5"  class="bluelink"> RoboCup@Home league 2016 </a></td>
          <td><a class="pub-authors fontsmall"> Leipzig, Germany </a></td>
          <td><a class="pub-authors fontsmall">3rd Place</a></td>
          <td><a class="pub-authors fontsmall">Major</a></td>
        </tr>
        <tr>
          <td><a href="https://www.robocup.org/events/14"  class="bluelink"> Pre-RoboCup Asia-Pacific Competition </a></td>
          <td><a class="pub-authors fontsmall"> Beijing, China </a></td>
          <td><a class="pub-authors fontsmall">Champion</a></td>
          <td><a class="pub-authors fontsmall">Major</a></td>
        </tr>
        <tr>
          <td><a href="https://www.robocup.org/events/6"  class="bluelink"> RoboCup@Home league 2017 </a></td>
          <td><a class="pub-authors fontsmall"> Nagoya, Japan </a></td>
          <td><a class="pub-authors fontsmall">Best in Manipulation</a></td>
          <td><a class="pub-authors fontsmall">Major</a></td>
        </tr>
        <tr>
          <td><a href="http://robo-tend.ustc.edu.cn/results.html"  class="bluelink"> The IJCAI-2019 Eldercare Robot Challenges </a></td>
          <td><a class="pub-authors fontsmall"> Macau, China </a></td>
          <td><a class="pub-authors fontsmall">Champion</a></td>
          <td><a class="pub-authors fontsmall">Leader</a></td>
        </tr>
      </table>
      </p>
      
      <hr style="height:1px;border:none;border-top:1px dashed #0066CC;"/>
      
        <a href="https://optitrack.com/"  class="fontnorm bluelink"> <b>MoCap System</b></a> <a class="fontverysmall"><em>(2016 - 2018)</em></a><br>
        <p>
        for Testing: 
        <br>
        - <a href="pdf/Cleaning_Robots_Test.pdf" class="bluelink">Cleaning Robots Test</a><br>
        - <a href="http://roboticsbase.ustc.edu.cn/"  class="bluelink">Anhui Robot Technology Standard Innovation Base</a> 
        <br>
        for Calibration:
        <br>
        - <a href="http://staff.ustc.edu.cn/~wufeng02/doc/pdf/ZCWicira17.pdf"  class="bluelink">General Batch-Calibration Framework</a>
        <br>
        - <a href="#ref2" class="bluelink">RGB-D Cameras Calibration [ref.2]</a>
        <br>
        for Training:
        <br>
        - <a href="http://www.sohu.com/a/217691740_100071565"  class="bluelink">Real Simulation Unified Platform</a>
        <br>
      </p>
    </div>
    <hr color=#987cb9 SIZE=1 />
  </div>
  
  <div id="pubs">
    <div class="width25 leftfloat" align="left" id="pub_title">
      <br>
      <a class="fontbig blacklink">Publication</a>
    </div>
    <div class="width75 leftfloat linebigheight" align="left" id="pub_content">
      <p>
        <a class="fontlarge">All publications: </a>
        <a href="https://scholar.google.com/citations?user=h7vRddgAAAAJ&hl=zh-CN" class="fontlarge bluelink bb">Google Scholar</a>, 
        <!-- <a href="https://orcid.org/0000-0003-1888-9947" class="fontlarge bluelink bb">ORCID</a>,  -->
        <!-- <a href="https://publons.com/researcher/2932962/guangda-chen/" class="fontlarge bluelink bb">Publons</a>,  -->
        <a href="https://www.researchgate.net/profile/Guangda_Chen4" class="fontlarge bluelink bb">ResearchGate</a>.
        <a class="fontlarge"> Chinese patents: </a>
        <a href="https://www.patentguru.com/cn/search?inventor=%22%E9%99%88%E5%B9%BF%E5%A4%A7%22&assignee=%22%E7%BD%91%E6%98%93%28%E6%9D%AD%E5%B7%9E%29%E7%BD%91%E7%BB%9C%E6%9C%89%E9%99%90%E5%85%AC%E5%8F%B8%22%2C%22%E4%B8%AD%E5%9B%BD%E7%A7%91%E5%AD%A6%E6%8A%80%E6%9C%AF%E5%A4%A7%E5%AD%A6%22" 
        class="fontlarge bluelink bb">PatentGuru</a> 
      </p>
      <!-- <p>

      </p> -->
      <hr style="height:1px;border:none;border-top:1px dashed #0066CC;"/>
      <ol>
        <li>
          <p>      <div class="linebigheight" align="left" id="ref_excavator">
            <a href="https://ieeexplore.ieee.org/document/10356425"  class="fontnorm bluelink">Modeling and Control of General Hydraulic Excavator for
              Human-in-the-loop Automation</a>
            <br>
            <a class="pub-authors"><b>Guangda Chen</b>, Yinghao Gan, J. Chen, S. Shi, W. Chen, </a><a href="https://scholar.google.com/citations?user=SSBrkpMAAAAJ&hl=en"  class="bluelink">Y. Chen</a>, <a href="https://scholar.google.com/citations?hl=en&user=1hI9bqUAAAAJ"  class="bluelink">Rong Xiong</a> <a class="pub-authors">and Changjie Fan.</a>
            <br>
            <I><a href="https://ictai.computer.org/2023/"  class="bluelink pub-venue">ICTAI 2023</a></I> 
            (<a href="https://ccf.atom.im/"  class="bluelink ">CCF-C</a>)
            <br>
            <a class="bluelink" onclick="document.all.bib_excavator_conf.style.display=(document.all.bib_excavator_conf.style.display =='none')?'':'none'" >
              [BibTeX]</a>,
              <a class="bluelink" onclick="document.all.abstract_excavator_conf.style.display=(document.all.abstract_excavator_conf.style.display =='none')?'':'none'" >
                [Abstract]</a>, 
<a href="pdf/Modeling and Control of General Hydraulic Excavator for Human-in-the-loop Automation.pdf"  class="bluelink">[PDF]</a>, <a href="https://youtu.be/N6I0WZGSF68"  class="bluelink">[Demo]</a>
<div id="bib_excavator_conf" style="display:none" class="bibtexdiv">   
  @INPROCEEDINGS{10356425,<br>
    author={Chen, Guangda and Gan, Yinghao and Chen, Jiayi and Shi, Shuanwu and Chen, Wei and Chen, Yingfeng and Xiong, Rong and Fan, Changjie},<br>
    booktitle={2023 IEEE 35th International Conference on Tools with Artificial Intelligence (ICTAI)},<br>
    title={Modeling and Control of General Hydraulic Excavator for Human-in-the-loop Automation}, <br>
    year={2023},<br>
    pages={708-716},<br>
    doi={10.1109/ICTAI59109.2023.00110}}<br>     
</div>
<div id="abstract_excavator_conf" style="display:none" class="bibtexdiv">
  <b>Abstract:</b> As labor shortages and safety regulations become 
  more prominent, the need for human-in-the-loop automation 
  of excavators is increasing. To meet this demand, we have 
  developed a comprehensive modeling method for the excavator 
  arm using nonlinear optimization approaches, including a 
  simplified model that maps the task space to the joint space, 
  as well as an equivalent model that maps the joint space to 
  the actuator space. These models were then used to build a 
  feedforward-PID joint velocity controller and a joint trajectory 
  controller combined with position feedback, which forms the 
  core of our proposed semi-automatic control system for the 
  excavator arm. Our deployment scheme is simple and efficient, 
  and has been deployed on two excavators of different makes 
  and sizes. Experiments show that our deployment scheme 
  performs well on both excavators, with an average error of 
  0.05 rad/s for the velocity controller and less than 5 cm for 
  the trajectory controller. Using our semi-automatic system, we 
  have completed demonstration experiments for precise digging 
  and grading operations. A demonstration video can be found 
  at https://youtu.be/N6I0WZGSF68.
</div>           
</div></p>
        </li>
        <hr style="height:1px;border:none;border-top:1px dashed #0066CC;"/>
        <li>
          <p>      <div class="linebigheight" align="left" id="ref8">
            <a href="https://doi.org/10.27517/d.cnki.gzkju.2021.000357"  class="fontnorm bluelink">Robot Navigation in Complex and 
              Dynamic Pedestrian Scenarios<br>复杂动态行人场景下的机器人导航</a>
            <br>
            <a class="pub-authors"><b>Guangda Chen</b>.</a> <b><a class="award">PhD Thesis</a></b>
            <br>
            <I><a class="bluelink pub-venue">University of Science and Technology of China</a></I>. Hefei, China. June, 2021.
            <br>
            <a class="bluelink" onclick="document.all.child8.style.display=(document.all.child8.style.display =='none')?'':'none'" >
              [BibTeX]</a>, 
            <a href="pdf/phd_cgd.pdf"  class="bluelink">[PDF]</a>, <a href="pdf/phd_ppt.pdf"  class="bluelink">[Slides]</a>
            <div id="child8" style="display:none" class="bibtexdiv">
              @phdthesis{cgd-phd,<br>
                title = {复杂动态行人场景下的机器人导航},<br>
                author = {陈广大},<br>
                year = {2021},<br>
                school = {中国科学技术大学},<br>
                ADDRESS    = "合肥"<br>
              };          
            </div> 
          </div></p>
        </li>
        <hr style="height:1px;border:none;border-top:1px dashed #0066CC;"/>
        <li>
          <p>
            <div class="linebigheight" align="left" id="ref2">
              <a href="https://ieeexplore.ieee.org/document/8588983"  class="fontnorm bluelink">Accurate Intrinsic and Extrinsic Calibration of RGB-D Cameras with GP-based Depth Correction</a>
              <br>
              <a class="pub-authors"><b>Guangda Chen</b>, Guowei Cui, Zhongxiao Jin, </a><a href="http://staff.ustc.edu.cn/~wufeng02/"  class="bluelink">Feng Wu</a><a class="pub-authors"> and Xiaoping Chen.</a>
              <br>
              <I><a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=7361"  class="bluelink pub-venue"><b>IEEE Sensors Journal</b></a></I> 
              (<a href="https://www.caa.org.cn/Uploads/image/file/20230414/20230414170847_65463.pdf#page=15"  class="bluelink ">CAA-B</a>), 
              (Volume: 19 , <a href="https://ieeexplore.ieee.org/xpl/tocresult.jsp?isnumber=8662729"  class="bluelink">Issue: 7</a> , 
              April, 1, 2019. <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=7361"  class="bluelink"  target="_blank">
                IF: 4.3</a>)
              <br>
              <a id="main1" class="bluelink" onclick="document.all.child1.style.display=(document.all.child1.style.display =='none')?'':'none'" >
                [BibTeX]</a>, 
                <a class="bluelink" onclick="document.all.abs2.style.display=(document.all.abs2.style.display =='none')?'':'none'" >
                  [Abstract]</a>, 
                <a href="pdf/rgbd_cal_j.pdf"  class="bluelink">[PDF]</a>, 
                <a href="pdf/jsen.pdf"  class="bluelink">[PDF2]</a>
                <div id="child1" style="display:none" class="bibtexdiv">
                  @Article{chen2019accurate,<br>
                    title = {Accurate Intrinsic and Extrinsic Calibration of {RGB}-D Cameras With {GP}-Based Depth Correction},<br>
                    author = {Guangda Chen and Guowei Cui and Zhongxiao Jin and Feng Wu and Xiaoping Chen},<br>
                    journal = {IEEE Sensors Journal},<br>
                    volume = {19},<br>
                    number = {7},<br>
                    pages = {2685--2694},<br>
                    year = 2019,<br>
                    month = {apr},<br>
                    publisher = {IEEE},<br>
                    doi = {10.1109/jsen.2018.2889805},<br>
                    url = {https://doi.org/10.1109%2Fjsen.2018.2889805}<br>
                  } 
                </div> 
                <div id="abs2" style="display:none" class="bibtexdiv">
                  <b>Abstract:</b> In recent years, more and more robots have been
                  equipped with low-cost RGB-D sensors, such as Microsoft Kinect
                  and Intel Realsense, for safe navigation and active interaction
                  with objects and people. In order to obtain more accurate
                  and reliable fused color and depth information (coloured point
                  clouds), not only the intrinsic and extrinsic parameters of color
                  and depth sensor should be precisely calibrated, but also the
                  external corrections of depth measurements are required. In
                  this paper, using motion capture system, we propose a reliable
                  calibration framework that enables the precise estimation of the
                  intrinsic and extrinsic parameters of RGB-D sensors and provide
                  a model-free depth calibration method based on heteroscedastic
                  Gaussian Processes. Compared with the existing depth correction
                  techniques, our method can simultaneously estimate the mean
                  and variance of the depth error at different measurement
                  distances, i.e., the probability distribution of the depth error
                  relative to the measured distance, which is essential in the state
                  estimation problems. To verify the effectiveness of our approach,
                  we conduct a thorough qualitative and quantitative analysis of
                  the major steps of our calibration method, and compare our
                  experimental results with other related work. Furthermore, we
                  demonstrate an experiment about the overall improvement of
                  visual SLAM with a Kinect device calibrated by our calibration
                  technique.
                  <br>
                  <b>Keywords:</b> RGB-D cameras, calibration, motion capture system
                </div> 
            </div>
          </p>
        </li>
        <hr style="height:1px;border:none;border-top:1px dashed #0066CC;"/>
        <li>
          <p>
            <div class="linebigheight" align="left" id="ref1">
              <a href="https://ieeexplore.ieee.org/document/9238090"  class="fontnorm bluelink">Robot Navigation with Map-Based Deep Reinforcement Learning</a>
              <br>
              <a class="pub-authors"><b>Guangda Chen</b>, Lifan Pan, Y. C., P. X., Z. W., P. W., </a><a href="http://staff.ustc.edu.cn/~jianmin/"  class="bluelink">Jianmin Ji</a><a class="pub-authors"> and Xiaoping Chen. </a>
              <br>
              <I><a href="http://www.icnsc2020.org/"  class="bluelink pub-venue">ICNSC 2020</a></I> (<a href="https://www.caa.org.cn/Uploads/image/file/20230213/20230213163755_94080.pdf#page=21"  class="bluelink ">CAA-B</a>), <b><a class="award">Best Student Paper Award</a></b>
              <br><a id="main1" class="bluelink" onclick="document.all.child2.style.display=(document.all.child2.style.display =='none')?'':'none'" >
                [BibTeX]</a>, 
              <a class="bluelink" onclick="document.all.abs1.style.display=(document.all.abs1.style.display =='none')?'':'none'" >
                  [Abstract]</a>, 
              <a href="pdf/ICNSC_2020_paper_11.pdf"  class="bluelink">[PDF]</a>, <a href="https://youtu.be/Eq4AjsFH_cU"  class="bluelink">[Demo]</a>, <a href="pdf/ICNSC2020slide.pdf"  class="bluelink">[Slides]</a>, <a href="pdf/ICNSC2020Award.pdf"  class="bluelink">[Award]</a>
              <br>
              <div id="child2" style="display:none" class="bibtexdiv">
                @InProceedings{chen2020robot,<br>
                  title = {Robot Navigation with Map-Based Deep Reinforcement Learning},<br>
                  author = {Chen, Guangda and Pan, Lifan and Chen, Yu'an and Xu, Pei and Wang, Zhiqiang and Wu, Peichen and Ji, Jianmin and Chen, Xiaoping},<br>
                  year = {2020},<br>
                  pages = {1-6},<br>
                  address = {Nanjing, China},<br>
                  doi = {10.1109/ICNSC48988.2020.9238090},<br>
                  organization = {IEEE}<br>
                }
              </div> 
              <div id="abs1" style="display:none" class="bibtexdiv">
                <b>Abstract:</b> This paper proposes an end-to-end deep reinforcement learning approach for mobile robot navigation with
                dynamic obstacles avoidance. Using experience collected in a
                simulation environment, a convolutional neural network (CNN)
                is trained to predict proper steering actions of a robot from its
                egocentric local occupancy maps, which accommodate various
                sensors and fusion algorithms. The trained neural network is
                then transferred and executed on a real-world mobile robot to
                guide its local path planning. The new approach is evaluated
                both qualitatively and quantitatively in simulation and realworld robot experiments. The results show that the map-based
                end-to-end navigation model is easy to be deployed to a robotic
                platform, robust to sensor noise and outperforms other existing
                DRL-based models in many indicators.
                <br>
                <b>Keywords:</b> robot navigation, obstacle avoidance, reinforcement learning, occupancy map
              </div> 
            </div>
          </p>
        </li>
        <hr style="height:1px;border:none;border-top:1px dashed #0066CC;"/>
        <li>
          <p>
            <div class="linebigheight" align="left" id="refx2">
              <a href="https://link.springer.com/article/10.1007/s42979-021-00817-z"  class="fontnorm bluelink">Deep Reinforcement Learning of Map-Based Obstacle Avoidance for Mobile Robot Navigation</a>
              <br>
              <a class="pub-authors"><b>Guangda Chen</b>, Lifan Pan, Y. C., P. X., Z. W., P. W., Jianmin Ji and Xiaoping Chen.</a>
              <br>
              <I><a href="https://www.springer.com/journal/42979"  class="bluelink pub-venue"><b>SN Computer Science</b></a></I> (Volume: 2 , <a href="https://link.springer.com/journal/42979/volumes-and-issues/2-6"  class="bluelink">Issue: 6</a> , August, 18, 2021)
              <br>
              <a id="main1" class="bluelink" onclick="document.all.childx2.style.display=(document.all.childx2.style.display =='none')?'':'none'" >
                [BibTeX]</a>, <a class="bluelink" onclick="document.all.absx2.style.display=(document.all.absx2.style.display =='none')?'':'none'" >
                  [Abstract]</a>, <a href="pdf/Chen2021_Article_DeepReinforcementLearningOfMap.pdf"  class="bluelink">[PDF]</a>
              <br>
              <div id="childx2" style="display:none" class="bibtexdiv">
                @article{chen2021deep,<br>
                  title = {Deep Reinforcement Learning of Map-Based Obstacle Avoidance for Mobile Robot Navigation},<br>
                  author = {Chen, Guangda and Pan, Lifan and Chen, Yu'an and Xu, Pei and Wang, Zhiqiang and Wu, Peichen and Ji, Jianmin and Chen, Xiaoping},<br>
                  journal = {SN Computer Science},<br>
                  volume = {2},<br>
                  number = {6},<br>
                  pages={1--14},<br>
                  year = {2021},<br>
                  month = {August},<br>
                  publisher = {Springer},<br>
                  doi = {10.1007/s42979-021-00817-z},<br>
                  url = {https://doi.org/10.1007/s42979-021-00817-z}<br>
                } 
              </div>
              <div id="absx2" style="display:none" class="bibtexdiv">
                <b>Abstract:</b> Autonomous and safe navigation in complex environments without collisions is
                particularly important for mobile robots. In this paper, we propose an end-to-end deep reinforcement learning method for mobile robot navigation with map-based obstacle avoidance.
                Using the experience collected in the simulation environment, a convolutional neural network is trained to predict the proper steering operation of the robot based on its egocentric
                local grid maps, which can accommodate various sensors and fusion algorithms. We use dueling double DQN with prioritized experienced replay technology to update parameters of
                the network and integrate curriculum learning techniques to enhance its performance. The
                trained deep neural network is then transferred and executed on a real-world mobile robot to
                guide it to avoid local obstacles for long-range navigation. The qualitative and quantitative
                evaluations of the new approach were performed in simulations and real robot experiments.
                The results show that the end-to-end map-based obstacle avoidance model is easy to deploy,
                without any fine-tuning, robust to sensor noise, compatible with different sensors, and better
                than other related DRL-based models in many evaluation indicators.
                <br>
                <b>Keywords:</b> robot navigation, obstacle avoidance, deep reinforcement learning, grid map
              </div> 
            </div>
          </p>
        </li>
        <hr style="height:1px;border:none;border-top:1px dashed #0066CC;"/>
        <li>
          <p>      <div class="linebigheight" align="left" id="ref0">
            <a href="https://www.mdpi.com/1424-8220/20/17/4836"  class="fontnorm bluelink">Distributed Non-Communicating Multi-Robot Collision 
              Avoidance via Map-Based Deep Reinforcement Learning</a>
            <br>
            <a class="pub-authors"><b>Guangda Chen</b>, Shunyi Yao, Jun Ma, L. P., Y. C., P. X., </a><a href="http://staff.ustc.edu.cn/~jianmin/"  class="bluelink">Jianmin Ji</a><a class="pub-authors"> and Xiaoping Chen. </a>
            <br>
            <I><a href="https://www.mdpi.com/journal/sensors"  class="bluelink pub-venue"><b>Sensors</b></a></I>  
            (Volume: 20, <a href="https://www.mdpi.com/1424-8220/20/17"  class="bluelink">Issue: 17</a> , August, 27, 2020. 
            <a href="https://www.mdpi.com/journal/sensors"  class="bluelink"  target="_blank">IF: 3.4</a>)
            <br>
            <a id="main1" class="bluelink" onclick="document.all.child0.style.display=(document.all.child0.style.display =='none')?'':'none'" >
              [BibTeX]</a>, 
            <a class="bluelink" onclick="document.all.abs0.style.display=(document.all.abs0.style.display =='none')?'':'none'" >
                [Abstract]</a>, 
            <a href="pdf/DRL_NAV_sensors.pdf"  class="bluelink">[PDF]</a>, <a href="https://youtu.be/KOb1q23L7-U"  class="bluelink">[YouTube]</a>, <a href="https://www.bilibili.com/video/BV12f4y1Q7cx"  class="bluelink">[bili_1]</a>, <a href="https://www.bilibili.com/video/BV12v411C7sM"  class="bluelink">[bili_2]</a>
            <br>
            <div id="child0" style="display:none" class="bibtexdiv">
              @Article{chen2020distributed,<br>
                title = {Distributed Non-Communicating Multi-Robot Collision Avoidance via Map-Based Deep Reinforcement Learning},<br>
                author = {Chen, Guangda and Yao, Shunyi and Ma, Jun and Pan, Lifan and Chen, Yu'an and Xu, Pei and Ji, Jianmin and Chen, Xiaoping},<br>
                journal = {Sensors},<br>
                volume = {20},<br>
                number = {17},<br>
                pages = {4836},<br>
                year = {2020},<br>
                publisher = {Multidisciplinary Digital Publishing Institute},<br>
                doi = {10.3390/s20174836},<br>
                url = {https://www.mdpi.com/1424-8220/20/17/4836}<br>
              }
            </div> 
            <div id="abs0" style="display:none" class="bibtexdiv">
              <b>Abstract:</b> It is challenging to avoid obstacles safely and efficiently for multiple robots of different shapes in distributed and communication-free scenarios, 
              where robots do not communicate with each other and only sense other robots' positions and obstacles around them. 
              Most existing multi-robot collision avoidance systems either require communication between robots or require expensive 
              movement data of other robots, like velocities, accelerations and paths. In this paper, we propose a map-based deep reinforcement 
              learning approach for multi-robot collision avoidance in a distributed and communication-free environment. 
              We use the egocentric local grid map of a robot to represent the environmental information around it including its shape 
              and observable appearances of other robots and obstacles, which can be easily generated by using multiple sensors or sensor fusion. 
              Then we apply the distributed proximal policy optimization (DPPO) algorithm to train a convolutional neural network that directly 
              maps three frames of egocentric local grid maps and the robot's relative local goal positions into low-level robot control commands. 
              Compared to other methods, the map-based approach is more robust to noisy sensor data, does not require robots' movement data and 
              considers sizes and shapes of related robots, which make it to be more efficient and easier to be deployed to real robots. 
              We first train the neural network in a specified simulator of multiple mobile robots using DPPO, where a multi-stage curriculum 
              learning strategy for multiple scenarios is used to improve the performance. Then we deploy the trained model to real robots to 
              perform collision avoidance in their navigation without tedious parameter tuning. We evaluate the approach with multiple scenarios 
              both in the simulator and on four differential-drive mobile robots in the real world. Both qualitative and quantitative experiments 
              show that our approach is efficient and outperforms existing DRL-based approaches in many indicators. We also conduct ablation 
              studies showing the positive effects of using egocentric grid maps and multi-stage curriculum learning.
              <br>
              <b>Keywords:</b> multi-robot navigation, distributed collision avoidance, deep reinforcement learning
            </div> 
          </div></p>
        </li>
        <hr style="height:1px;border:none;border-top:1px dashed #0066CC;"/>
        <li>
          <p>      <div class="linebigheight" align="left" id="ref7">
            <a href="http://dx.doi.org/10.1109/IROS51168.2021.9636579"  class="fontnorm bluelink">Crowd-Aware Robot Navigation for Pedestrians with Multiple Collision
              Avoidance Strategies via Map-based Deep Reinforcement Learning</a>
            <br>
            <a class="pub-authors">Shunyi Yao∗, <b>Guangda Chen∗</b>, Quecheng Qiu, Jun Ma, Xiaoping Chen and Jianmin Ji.</a>
            <br>
            <I><a href="https://www.iros2021.org/"  class="bluelink pub-venue">IROS 2021</a></I> (<a href="https://numbda.cs.tsinghua.edu.cn/~yuwj/TH-CPL.pdf#page=12"  class="bluelink ">TH-B</a>)
            <br>
            <a id="main1" class="bluelink" onclick="document.all.drl_nav_iros_bib.style.display=(document.all.drl_nav_iros_bib.style.display =='none')?'':'none'" >
              [BibTeX]</a>,
            <a class="bluelink" onclick="document.all.abs7.style.display=(document.all.abs7.style.display =='none')?'':'none'" >
                [Abstract]</a>, 
            <a href="https://www.bilibili.com/video/BV1Vb4y1D7R6"  class="bluelink">[Demo]</a>, 
            <a href="pdf/IROS_2021_PedNav.pdf"  class="bluelink">[PDF]</a>
            <br>
            <div id="drl_nav_iros_bib" style="display:none" class="bibtexdiv">
              @inproceedings{yao2021crowd,<br>
                title={Crowd-aware robot navigation for pedestrians with multiple collision avoidance strategies via map-based deep reinforcement learning},<br>
                author={Yao, Shunyi and Chen, Guangda and Qiu, Quecheng and Ma, Jun and Chen, Xiaoping and Ji, Jianmin},<br>
                booktitle={2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},<br>
                pages={8144--8150},<br>
                year={2021},<br>
                organization={IEEE}<br>
              }<br>
            </div>
            <div id="abs7" style="display:none" class="bibtexdiv">
              <b>Abstract:</b> It is challenging for a mobile robot to navigate
              through human crowds. Existing approaches usually assume
              that pedestrians follow a predefined collision avoidance strategy,
              like social force model (SFM) or optimal reciprocal collision
              avoidance (ORCA). However, their performances commonly
              need to be further improved for practical applications, where
              pedestrians follow multiple different collision avoidance strategies. In this paper, we propose a map-based deep reinforcement
              learning approach for crowd-aware robot navigation with
              various pedestrians. We use the sensor map to represent the
              environmental information around the robot, including its shape
              and observable appearances of obstacles. We also introduce
              the pedestrian map that specifies the movements of pedestrians
              around the robot. By applying both maps as inputs of the
              neural network, we show that a navigation policy can be trained
              to better interact with pedestrians following different collision
              avoidance strategies. We evaluate our approach under multiple
              scenarios both in the simulator and on an actual robot. The
              results show that our approach allows the robot to successfully
              interact with various pedestrians and outperforms compared
              methods in terms of the success rate.
            </div> 
          </div></p>
        </li>
        <hr style="height:1px;border:none;border-top:1px dashed #0066CC;"/>
        <li>
          <p>      <div class="linebigheight" align="left" id="ref4">
            <a href="https://ieeexplore.ieee.org/abstract/document/9288300/"  class="fontnorm bluelink">Multi-Robot Collision Avoidance with Map-Based Deep Reinforcement Learning</a>
            <br>
            <a class="pub-authors">Shunyi Yao∗, <b>Guangda Chen∗</b>, Lifan Pan, Jun Ma, <a href="http://staff.ustc.edu.cn/~jianmin/"  class="bluelink">Jianmin Ji</a> and Xiaoping Chen. </a>
            <br>
            <I><a href="https://ictai2020.org/index.html"  class="bluelink pub-venue">ICTAI 2020</a></I> (<a href="https://ccf.atom.im/"  class="bluelink ">CCF-C</a>)
            <br><a id="main1" class="bluelink" onclick="document.all.child4.style.display=(document.all.child4.style.display =='none')?'':'none'" >
              [BibTeX]</a>, 
            <a class="bluelink" onclick="document.all.abs4.style.display=(document.all.abs4.style.display =='none')?'':'none'" >
                [Abstract]</a>, 
            <a href="pdf/ICTAI_2020.pdf"  class="bluelink">[PDF]</a>, <a href="https://youtu.be/jcLKlEXuFuk"  class="bluelink">[Demo]</a>
            <br>
            <div id="child4" style="display:none" class="bibtexdiv">
              @InProceedings{yao2020multi,<br>
                author = {Yao, Shunyi and Chen, Guangda and Pan, Lifan and Ma, Jun and Ji, Jianmin and Chen, Xiaoping},<br>
                booktitle = {Proceedings of the 32th International Conference on Tools with Artificial Intelligence (ICTAI)}, <br>
                title = {Multi-Robot Collision Avoidance with Map-based Deep Reinforcement Learning}, <br>
                pages = {532--539}, <br>
                year = {2020}, <br>
                organization = {IEEE} <br>
              }
            </div> 
            <div id="abs4" style="display:none" class="bibtexdiv">
              <b>Abstract:</b> Multi-robot collision avoidance in a communicationfree environment is one of the key issues for mobile robotics
              and autonomous driving. In this paper, we propose a mapbased deep reinforcement learning (DRL) approach for collision
              avoidance of multiple robots, where robots do not communicate
              with each other and only sense other robots' positions and
              the obstacles around them. We use the egocentric grid map
              of a robot to represent the environmental information around
              it, which can be easily generated by using multiple sensors
              or sensor fusion. The learned policy generated from the DRL
              model directly maps 3 frames of egocentric grid maps and
              the robot's relative local goal positions into low-level robot
              control commands. Compared to other methods, the map-based
              approach is more robust to noisy sensor data and does not require
              the expensive movement data of other robots, like velocities,
              accelerations and paths. We first train a convolutional neural
              network for the navigation policy in a simulator of multiple
              mobile robots using proximal policy optimization (PPO), where
              curriculum learning strategy is used to accelerate the training
              process. Then we deploy the trained model to real robots to
              perform collision avoidance in their navigation. We evaluate the
              approach with various scenarios both in the simulator and on
              three differential-drive mobile robots in the real world. Both
              qualitative and quantitative experiments show that our approach
              is efficient with high success rate. The demonstration video can
              be found at https://youtu.be/jcLKlEXuFuk.
              <br>
              <b>Keywords:</b> multi-robots collision avoidance, reinforcement learning, egocentric grid map
            </div> 
          </div></p>
        </li>
      </ol>
      <p>
        <b>*</b> These authors contributed equally to the work.
        </p>
      <hr style="height:1px;border:none;border-top:1px dashed #0066CC;"/>
      <a class="award">授权专利</a>:
        <ol>
        <li>
          <p>      <div class="linebigheight" align="left" id="ref7">
            <a href="https://www.patentguru.com/cn/CN112304314B"  class="fontnorm bluelink">一种分布式多机器人的导航方法</a> 

          </div></p>
        </li>
        </ol>
    </div>
    <hr color=#987cb9 SIZE=1 />
  </div>

  <div id="service">
    <div class="width25 leftfloat" align="left" id="service_title">
      <br>
      <a class="fontbig blacklink">Other</a>
    </div>
    <div class="width75 leftfloat linebigheight" align="left" id="service_content">
      <p>
      Conference reviewer : 
      <a href="https://www.iros2019.org/"  class="fontnorm bluelink fontverysmall bb"> IROS 2019</a>, 


      <a href="http://www.icra2021.org/"  class="fontnorm bluelink fontverysmall bb"> ICRA 2021</a>,  
      <a href="http://ieeesmc2021.org/"  class="fontnorm bluelink fontverysmall bb"> SMC 2021</a>, 
      <a href="https://www.icra2022.org/"  class="fontnorm bluelink fontverysmall bb"> ICRA 2022</a>, 
      <a href="https://iros2022.org/"  class="fontnorm bluelink fontverysmall bb"> IROS 2022</a>, 
      <a href="https://www.icra2023.org/"  class="fontnorm bluelink fontverysmall bb"> ICRA 2023</a>, 
      <a href="https://ieee-iros.org/"  class="fontnorm bluelink fontverysmall bb"> IROS 2023</a>
      <br>
      </p>
      <p>
        Journal reviewer : 
        <a href="https://journals.sagepub.com/home/ijr"  class="fontnorm bluelink fontverysmall bb"> International Journal of Robotics Research (IJRR)</a>, 
        <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=100"  class="fontnorm bluelink fontverysmall bb"> IEEE Robotics & Automation Magazine</a>, 
        <a href="https://www.sciencedirect.com/journal/automation-in-construction"  class="fontnorm bluelink fontverysmall bb"> Automation in Construction</a>,
        <a href="https://www.sciencedirect.com/journal/computers-and-electronics-in-agriculture"  class="fontnorm bluelink fontverysmall bb"> Computers and Electronics in Agriculture</a>,
        <a href="https://www.sciencedirect.com/journal/expert-systems-with-applications"  class="fontnorm bluelink fontverysmall bb"> Expert Syst Appl</a>,
        <a href="https://www.sciencedirect.com/journal/engineering-applications-of-artificial-intelligence"  class="fontnorm bluelink fontverysmall bb"> Engineering Applications of Artificial Intelligence</a>, 
        <a href="https://www.sciencedirect.com/journal/robotics-and-autonomous-systems"  class="fontnorm bluelink fontverysmall bb"> Robotics and Autonomous Systems</a>, 
        <a href="https://www.journals.elsevier.com/applied-soft-computing"  class="fontnorm bluelink fontverysmall bb"> Applied Soft Computing</a>, 
        <a href="https://www.mdpi.com/journal/remotesensing"  class="fontnorm bluelink fontverysmall bb"> Remote Sensing</a>, 
        <a href="https://www.frontiersin.org/journals/neurorobotics"  class="fontnorm bluelink fontverysmall bb"> Frontiers in Neurorobotics</a>, 
        <a href="https://www.sciencedirect.com/journal/journal-of-rail-transport-planning-and-management"  class="fontnorm bluelink fontverysmall bb"> J Rail Transp Plann & Manage</a>, 
        <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=6287639"  class="fontnorm bluelink fontverysmall bb"> IEEE Access</a>, 
        <a href="https://www.mdpi.com/journal/bioengineering"  class="fontnorm bluelink fontverysmall bb"> Bioengineering</a>, 
        <a href="https://www.mdpi.com/journal/sensors"  class="fontnorm bluelink fontverysmall bb"> Sensors</a>, 
        <a href="https://www.mdpi.com/journal/sustainability"  class="fontnorm bluelink fontverysmall bb"> Sustainability</a>, 
        <a href="https://www.tandfonline.com/journals/ccos20"  class="fontnorm bluelink fontverysmall bb"> Connect. Sci.</a>, 
        <a href="https://www.hindawi.com/journals/complexity/"  class="fontnorm bluelink fontverysmall bb"> Complexity</a>, 
        <a href="https://www.tandfonline.com/action/journalInformation?show=journalMetrics&journalCode=uaai20"  class="fontnorm bluelink fontverysmall bb"> Appl. Artif. Intell.</a>, 
        <a href="https://www.mdpi.com/journal/processes"  class="fontnorm bluelink fontverysmall bb"> Processes</a>, 
        <a href="https://www.mdpi.com/journal/symmetry"  class="fontnorm bluelink fontverysmall bb"> Symmetry</a>, 
        <a href="https://www.mdpi.com/journal/genes"  class="fontnorm bluelink fontverysmall bb"> Genes</a>, 
        <a href="https://www.mdpi.com/journal/BDCC"  class="fontnorm bluelink fontverysmall bb"> Big Data Cogn. Comput.</a>, 
        <a href="https://benthamscience.com/journal/33/indexing-agency"  class="fontnorm bluelink fontverysmall bb"> Current Medical Imaging</a>, 
        <a href="https://www.sciencedirect.com/journal/cognitive-robotics"  class="fontnorm bluelink fontverysmall bb"> Cognitive Robotics</a>
        <br>
        </p>
      <p>
      Links : 
      <!-- <a href="https://www.douban.com/doulist/46245195/"  class="fontnorm bluelink">Favorite movies</a>,  -->
      <a href="http://arxivdaily.com/topic/topic-detail/148"  class="fontnorm bluelink fontverysmall bb">arxivdaily.cs.RO</a>, 
      <a href="https://git.ustc.edu.cn/"  class="fontnorm bluelink fontverysmall bb">USTC GitLab</a>, 
      <a href="https://orcid.org/0000-0002-9502-1204"  class="fontnorm bluelink fontverysmall bb">Song-Jia Yi's ORCID</a>
      <br>
      </p>
    </div>
    <hr color=#987cb9 SIZE=2 />
  </div>
  
<div>
  <a href="https://info.flagcounter.com/S8jX"><img src="https://s01.flagcounter.com/countxl/S8jX/bg_FFFFFF/txt_000000/border_CCCCCC/columns_2/maxflags_10/viewers_0/labels_1/pageviews_1/flags_0/percent_0/" alt="Flag Counter" border="0"></a>
  <div id="disclaimers" style="display:none" class="bibtexdiv">
    &emsp;The material provided on this website is to ensure the timely dissemination of academic and technical work. 
    Copyright and all rights therein are retained by authors or by other copyright holders. 
    All people copying this information are expected to adhere to the terms and constraints invoked by each author's copyright. 
    In most cases, these works may not be reposted without the explicit permission of the copyright holder.
  </div> 
  <div class="maxwidth" align="center" id="cpright">
    <p><a>&copy; </a>
    <a href="https://cgdsss.github.io/" class="bluelink">Guangda Chen</a> 2019-2024. 
    <a onclick="document.all.disclaimers.style.display=(document.all.disclaimers.style.display =='none')?'':'none'" class="bluelink">Click to read the disclaimer</a></p>
  </div>
</div>
<!-- <script type="text/javascript" src="./main.js"></script> -->
<script>
  // 当网页向下滑动 20px 出现"返回顶部" 按钮
  window.onscroll = function() {scrollFunction()};
   
  function scrollFunction() {console.log(121);
      if (document.body.scrollTop > 20 || document.documentElement.scrollTop > 20) {
          document.getElementById("myBtn").style.display = "block";
      } else {
          document.getElementById("myBtn").style.display = "none";
      }
  }
   
  // 点击按钮，返回顶部
  function topFunction() {
      document.body.scrollTop = 0;
      document.documentElement.scrollTop = 0;
  }
  </script>
<script type="text/javascript">
function get_width()
{
  screen_w = window.screen.availWidth;
  if (screen_w > 1000)
  {
    return 1000;
  }
  else
  {
    return screen_w;
  }
}
document.getElementById("page_size").style = "width:"+get_width() +"px";
if (get_width() < 1000) {

  maxline = "maxwidth leftfloat";
  document.getElementById("title_div_left").className = maxline;
  document.getElementById("imglove").src = "./pic/b.jpg";
  document.getElementById("title_div_right").className = maxline;
  // document.getElementById("imglove").style = "";
  document.getElementById("title_div_right").align = "left";
  document.getElementById("about_title").className = maxline;
  document.getElementById("about_content").className = maxline;
  document.getElementById("employ_title").className = maxline;
  document.getElementById("employ_content").className = maxline;
  document.getElementById("edu_title").className = maxline;
  document.getElementById("edu_content").className = maxline;
  document.getElementById("project_title").className = maxline;
  document.getElementById("project_content").className = maxline;
  document.getElementById("pub_title").className = maxline;
  document.getElementById("pub_content").className = maxline;
  document.getElementById("service_title").className = maxline;
  document.getElementById("service_content").className = maxline;
  
  var e = document.getElementById("about_title");
  e.innerHTML = e.innerHTML.replace('<br>', '');
  // e = document.getElementById("title_br");
  // e.innerHTML = e.innerHTML.replace('<br>', '');
  var e = document.getElementById("img_br");
  e.innerHTML = e.innerHTML.replace('<br><br><br>', '');
  var e = document.getElementById("employ_title");
  e.innerHTML = e.innerHTML.replace('<br>', '');
  var e = document.getElementById("edu_title");
  e.innerHTML = e.innerHTML.replace('<br>', '');
  var e = document.getElementById("project_title");
  e.innerHTML = e.innerHTML.replace('<br>', '');
  var e = document.getElementById("pub_title");
  e.innerHTML = e.innerHTML.replace('<br>', '');
  var e = document.getElementById("service_title");
  e.innerHTML = e.innerHTML.replace('<br>', '');
}
</script>
</div>
</div>
</div>
</body>
</html>
