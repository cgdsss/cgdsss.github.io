<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<title>cgdsss</title>
<link rel="shortcut icon" href="favicon.png"/>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link rel="stylesheet" type="text/css" href="./main.css">
<!-- <style type="text/css">
</style> -->
<link rel="shortcut icon" href="./pic/ustc.ico">
</head>
<body>
  <script>
    $.get("https://api.ipdata.co?api-key=test", function (response) {
      alert(response.country_code);
  }, "jsonp");
  </script>
<script type="text/javascript"> 
  window.location.href="https://cgdsss.gitee.io/cgd"; 
</script>
<button onclick="topFunction()" id="myBtn" title="top"></button>
<div align="center" class="backgrounddiv">
<div id = "page_size" style="width:1000px" >
<div class="divpad fontgrounddiv shadow">
<div>
  <div class="maxwidth leftfloat" align="left">
    <p>
      <a class="fontverybig blacklink" href="/"><b>Guangda Chen</b></a>
      <a itemprop="sameAs" content="https://orcid.org/0000-0003-1888-9947" href="https://orcid.org/0000-0003-1888-9947" target="orcid.widget" rel="noopener noreferrer" style="vertical-align:top;"><img src="https://orcid.org/sites/default/files/images/orcid_16x16.png" style="width:1em;margin-right:.5em;" alt="ORCID iD icon"></a>
      <!-- <br> -->
      <!-- <div id = "title_br"><br></div> -->
    </p>
  </div>
</div>
<div id = "title_div">
  <div class="width70 leftfloat" align="left" id = "title_div_left">
    <p>
      <a><b>Address : </b></a>
      <br>
      West Campus, USTC, 
       Hefei, Anhui, 230027, China.<a href="https://surl.amap.com/ljqmiQ184O2" id="icon" ><img src="./pic/gmap.ico" /></a>
      <br><br>
      <a><b>Email: </b></a>
      <br>
      <a href=mailto:cgdsss@mail.ustc.edu.cn class="bluelink" ">cgdsss@mail.ustc.edu.cn</a>
      <br><br>
      <a style="line-height:1.8;"><b>Navigator: </b></a>
      <br>
      <a href="#about" class="fontlarge bluelink">[About]</a> 
      <a href="#edu" class="fontlarge bluelink">[Education]</a>  
      <a href="#pubs" class="fontlarge bluelink">[Publication]</a>
      <a href="#project" class="fontlarge bluelink">[Project]</a>
    </p>
  </div>
  <div class="width30 leftfloat" align="center" id = "title_div_right">
    <a href="./day.html" id="icon" ><img id="imglove" style="width: 100%" src="./pic/me.jpg" /></a>
    <!-- <a id="wordlove" style="font-family: Comic Sans MS; font-size: 1.1em;"> <I>My sally girl, My love</I></a> -->
  <div id="img_br">
    <br><br><br>
  </div>
  </div>
  <hr color=#987cb9 SIZE=2 />
</div>

<div id="about">
  <div class="width25 leftfloat" align="left" id="about_title">
    <br>
    <a class="fontbig">About</a>
  </div>
  <div class="width75 leftfloat linebigheight" align="left" id="about_content">
    <p style="text-align:justify;">
    Mr. Guangda Chen is a Ph.D. candidate (BA17011, advised by Prof. 
    <a href="http://ai.ustc.edu.cn/en/people/xpchen.php"  class="bluelink">Xiaoping Chen</a>) in the 
    <a href="http://ai.ustc.edu.cn/" class="bluelink" >USTC Robotics Laboratory</a> at 
    the <a href="http://cs.ustc.edu.cn/" class="bluelink" >School of Computer Science and Technology</a>, 
    <a href="https://www.ustc.edu.cn/" class="bluelink" >University of Science and Technology of China </a>(USTC). 
    He received a Bachelor of Administration from <a href="http://www.cmu.edu.cn/"  class="bluelink">China Medical 
      University</a> in 2014. His research focuses on mobile robot navigation in dynamic and crowded environments, and interests 
      include Reinforcement Learning, Computer Vision and Calibration.
    <br><br>
    Find him on:
    <br>
    <a href="https://github.com/cgdsss"   id="icon" title="GitHub"><img src="./pic/github.ico" /></a>
    <a href="https://scholar.google.com/citations?user=h7vRddgAAAAJ&hl=zh-CN" id="icon"  title="Google Scholar"><img src="./pic/gs.ico" /></a>
    <a href="https://www.douban.com/people/cgdsss/"   title="豆瓣" id="icon"><img src="./pic/douban.ico" /></a>
    <a href="http://weibo.com/cgdsss"   title="新浪微博" id="icon"><img src="./pic/weibo.ico" /></a>
    <a href="https://music.163.com/#/user/home?id=430012241"   title="网易云音乐" id="icon"><img src="./pic/music.ico" /></a>
    <a href="https://www.linkedin.com/in/%E5%B9%BF%E5%A4%A7-%E9%99%88-1b0770109/"   id="icon" title="Linkedin"><img src="./pic/link.ico" /></a>
    <a href="https://www.facebook.com/guangda.chen.9"   title="facebook" id="icon"><img src="./pic/facebook.ico" /></a>
    <a href="https://twitter.com/cgdsss_USTC"   title="Twitter" id="icon"><img src="./pic/twitter.ico" /></a>
    <a href="https://www.instagram.com/guangdachen/"   title="instagram" id="icon"><img src="./pic/ins.ico" /></a>
    <a href="https://plus.google.com/u/0/100646927760929911525"   title="Google+" id="icon"><img src="./pic/googleplus.ico" /></a>
    </p>
  </div>
  <hr color=#987cb9 SIZE=1 />
</div>

<div id="edu">
  <div class="width25 leftfloat" align="left" id="edu_title">
    <br>
    <a class="fontbig">Education</a>
  </div>
  <div class="width75 leftfloat linebigheight" align="left" id="edu_content">
    <p>
    <a href="https://www.ustc.edu.cn/"  class="fontnorm bluelink"> University of Science and Technology of China</a>, 
    <a class="fontsmall">PhD</a>
    <br>
    - <a href="http://cs.ustc.edu.cn/"  class="bluelink"> Computer Application Technology</a>, 
    <a class="fontsmall"><em>2015 — now</em></a>
    <br>
    </p>
    <p>
    <a href="http://www.cmu.edu.cn/"  class="fontnorm bluelink"> China Medical University</a>, 
    <a class="fontsmall">BAdmin</a>
    <br>
    - <a href="http://202.118.40.32/dmi/"  class="bluelink"> Medical Informatics</a>, 
    <a class="fontsmall"><em>2011 — 2014</em></a>
    <br>
    - <a href="http://school.cmudental.com/"  class="bluelink"> Stomatology</a>, 
    <a class="fontsmall"><em>2010 — 2011</em></a>
    <br>
    </p>
  </div>
  <hr color=#987cb9 SIZE=1 />
</div>

<div id="pubs">
  <div class="width25 leftfloat" align="left" id="pub_title">
    <br>
    <a class="fontbig blacklink">Publication</a>
  </div>
  <div class="width75 leftfloat linebigheight" align="left" id="pub_content">
    <p>
      <a class="fontlarge">All publications: </a>
      <a href="https://scholar.google.com/citations?user=h7vRddgAAAAJ&hl=zh-CN" class="fontlarge bluelink">[Google Scholar]</a> 
      <a href="https://orcid.org/0000-0003-1888-9947" class="fontlarge bluelink">[ORCID]</a> 
      <a href="https://publons.com/researcher/2932962/guangda-chen/" class="fontlarge bluelink">[Publons]</a> 
      <a href="https://www.researchgate.net/profile/Guangda_Chen4" class="fontlarge bluelink">[ResearchGate]</a>
    </p>
    <div class="width4 leftfloat" align="left">
      [1]
    </div>
    <div class="width96 linebigheight" align="left" id="ref0">
      <a href="https://www.mdpi.com/1424-8220/20/17/4836"  class="fontnorm bluelink">Distributed Non-Communicating Multi-Robot Collision 
        Avoidance via Map-Based Deep Reinforcement Learning</a>
      <br>
      <a class="pub-authors"><b>Guangda Chen</b>, Shunyi Yao, Jun Ma, L. P., Y. C., P. X., </a><a href="http://staff.ustc.edu.cn/~jianmin/"  class="bluelink">Jianmin Ji</a><a class="pub-authors"> and Xiaoping Chen. </a>
      <br>
      <I><a href="https://www.mdpi.com/journal/sensors"  class="bluelink pub-venue"><b>Sensors</b></a></I>  (Volume: 20, <a href="https://www.mdpi.com/1424-8220/20/17"  class="bluelink">Issue: 17</a> , August, 27, 2020. <a href="http://www.greensci.net/search?kw=1424-8220"  class="bluelink">IF: 3.275</a>).
      <br>
      <a id="main1" class="bluelink" onclick="document.all.child0.style.display=(document.all.child0.style.display =='none')?'':'none'" >
        [BibTeX]</a>, 
      <a class="bluelink" onclick="document.all.abs0.style.display=(document.all.abs0.style.display =='none')?'':'none'" >
          [Abstract]</a>, 
      <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7506975/"  class="bluelink">[PMC]</a>, 
      <a href="pdf/sensors-20-04836.pdf"  class="bluelink">[PDF]</a>, <a href="pdf/Acceptance-Certificate-sensors-882109.pdf"  class="bluelink">[Certificate]</a>, <a href="https://youtu.be/KOb1q23L7-U"  class="bluelink">[YouTube]</a>, <a href="https://www.bilibili.com/video/BV12f4y1Q7cx"  class="bluelink">[bili_1]</a>, <a href="https://www.bilibili.com/video/BV12v411C7sM"  class="bluelink">[bili_2]</a>
      <br>
      <div id="child0" style="display:none" class="bibtexdiv">
        @Article{chen2020distributed,<br>
          title = {Distributed Non-Communicating Multi-Robot Collision Avoidance via Map-Based Deep Reinforcement Learning},<br>
          author = {Chen, Guangda and Yao, Shunyi and Ma, Jun and Pan, Lifan and Chen, Yu'an and Xu, Pei and Ji, Jianmin and Chen, Xiaoping},<br>
          journal = {Sensors},<br>
          volume = {20},<br>
          number = {17},<br>
          pages = {4836},<br>
          year = {2020},<br>
          publisher = {Multidisciplinary Digital Publishing Institute},<br>
          doi = {10.3390/s20174836},<br>
          url = {https://www.mdpi.com/1424-8220/20/17/4836}<br>
        }
      </div> 
      <div id="abs0" style="display:none" class="bibtexdiv">
        <b>Abstract:</b> It is challenging to avoid obstacles safely and efficiently for multiple robots of different shapes in distributed and communication-free scenarios, 
        where robots do not communicate with each other and only sense other robots' positions and obstacles around them. 
        Most existing multi-robot collision avoidance systems either require communication between robots or require expensive 
        movement data of other robots, like velocities, accelerations and paths. In this paper, we propose a map-based deep reinforcement 
        learning approach for multi-robot collision avoidance in a distributed and communication-free environment. 
        We use the egocentric local grid map of a robot to represent the environmental information around it including its shape 
        and observable appearances of other robots and obstacles, which can be easily generated by using multiple sensors or sensor fusion. 
        Then we apply the distributed proximal policy optimization (DPPO) algorithm to train a convolutional neural network that directly 
        maps three frames of egocentric local grid maps and the robot's relative local goal positions into low-level robot control commands. 
        Compared to other methods, the map-based approach is more robust to noisy sensor data, does not require robots' movement data and 
        considers sizes and shapes of related robots, which make it to be more efficient and easier to be deployed to real robots. 
        We first train the neural network in a specified simulator of multiple mobile robots using DPPO, where a multi-stage curriculum 
        learning strategy for multiple scenarios is used to improve the performance. Then we deploy the trained model to real robots to 
        perform collision avoidance in their navigation without tedious parameter tuning. We evaluate the approach with multiple scenarios 
        both in the simulator and on four differential-drive mobile robots in the real world. Both qualitative and quantitative experiments 
        show that our approach is efficient and outperforms existing DRL-based approaches in many indicators. We also conduct ablation 
        studies showing the positive effects of using egocentric grid maps and multi-stage curriculum learning.
        <br>
        <b>Keywords:</b> multi-robot navigation, distributed collision avoidance, deep reinforcement learning
      </div> 
      <br>
    </div>
    <div class="width4 leftfloat" align="left">
      [2]
    </div>
    <div class="width96 linebigheight" align="left" id="ref4">
      <a href="pdf/ICTAI_2020.pdf"  class="fontnorm bluelink">Multi-Robot Collision Avoidance with Map-Based Deep Reinforcement Learning</a>
      <br>
      <a class="pub-authors">Shunyi Yao∗, <b>Guangda Chen∗</b>, Lifan Pan, Jun Ma, <a href="http://staff.ustc.edu.cn/~jianmin/"  class="bluelink">Jianmin Ji</a> and Xiaoping Chen. </a>
      <br>
      <I><a href="https://ictai2020.org/index.html"  class="bluelink pub-venue">ICTAI 2020</a></I>, CCF C, Accepted
      <br><a id="main1" class="bluelink" onclick="document.all.child4.style.display=(document.all.child4.style.display =='none')?'':'none'" >
        [BibTeX]</a>, 
      <a class="bluelink" onclick="document.all.abs4.style.display=(document.all.abs4.style.display =='none')?'':'none'" >
          [Abstract]</a>, 
      <a href="pdf/ICTAI_2020.pdf"  class="bluelink">[PDF]</a>, <a href="https://youtu.be/jcLKlEXuFuk"  class="bluelink">[Demo]</a>
      <br>
      <div id="child4" style="display:none" class="bibtexdiv">
        @InProceedings{yao2020multi,<br>
          author={Yao, Shunyi and Chen, Guangda and Pan, Lifan and Ma, Jun and Ji, Jianmin and Chen, Xiaoping},<br>
          booktitle={Proceedings of the 32th International Conference on Tools with Artificial Intelligence (ICTAI)}, <br>
          title={Multi-Robot Collision Avoidance with Map-based Deep Reinforcement Learning}, <br>
          year={2020}<br>
        }
      </div> 
      <div id="abs4" style="display:none" class="bibtexdiv">
        <b>Abstract:</b> Multi-robot collision avoidance in a communicationfree environment is one of the key issues for mobile robotics
        and autonomous driving. In this paper, we propose a mapbased deep reinforcement learning (DRL) approach for collision
        avoidance of multiple robots, where robots do not communicate
        with each other and only sense other robots’ positions and
        the obstacles around them. We use the egocentric grid map
        of a robot to represent the environmental information around
        it, which can be easily generated by using multiple sensors
        or sensor fusion. The learned policy generated from the DRL
        model directly maps 3 frames of egocentric grid maps and
        the robot’s relative local goal positions into low-level robot
        control commands. Compared to other methods, the map-based
        approach is more robust to noisy sensor data and does not require
        the expensive movement data of other robots, like velocities,
        accelerations and paths. We first train a convolutional neural
        network for the navigation policy in a simulator of multiple
        mobile robots using proximal policy optimization (PPO), where
        curriculum learning strategy is used to accelerate the training
        process. Then we deploy the trained model to real robots to
        perform collision avoidance in their navigation. We evaluate the
        approach with various scenarios both in the simulator and on
        three differential-drive mobile robots in the real world. Both
        qualitative and quantitative experiments show that our approach
        is efficient with high success rate. The demonstration video can
        be found at https://youtu.be/jcLKlEXuFuk.
        <br>
        <b>Keywords:</b> multi-robots collision avoidance, reinforcement learning, egocentric grid map
      </div> 
      <br>
    </div>

    <div class="width4 leftfloat" align="left">
      [3]
    </div>
    <div class="width96 linebigheight" align="left" id="ref1">
      <a href="https://ieeexplore.ieee.org/document/9238090"  class="fontnorm bluelink">Robot Navigation with Map-Based Deep Reinforcement Learning</a>
      <br>
      <a class="pub-authors"><b>Guangda Chen</b>, Lifan Pan, Yu'an Chen, P. X., Z. W., P. W., </a><a href="http://staff.ustc.edu.cn/~jianmin/"  class="bluelink">Jianmin Ji</a><a class="pub-authors"> and Xiaoping Chen. </a>
      <br>
      <I><a href="http://www.icnsc2020.org/"  class="bluelink pub-venue">ICNSC 2020</a></I>, [<b><a class="award">Best Student Paper Award</a></b>]
      <br><a id="main1" class="bluelink" onclick="document.all.child2.style.display=(document.all.child2.style.display =='none')?'':'none'" >
        [BibTeX]</a>, 
      <a class="bluelink" onclick="document.all.abs1.style.display=(document.all.abs1.style.display =='none')?'':'none'" >
          [Abstract]</a>, 
      <a href="pdf/ICNSC_2020_paper_11.pdf"  class="bluelink">[PDF]</a>, <a href="https://arxiv.org/abs/2002.04349"  class="bluelink">[arXiv]</a>, <a href="https://youtu.be/Eq4AjsFH_cU"  class="bluelink">[Demo]</a>, <a href="pdf/ICNSC2020slide.pdf"  class="bluelink">[Slides]</a>, <a href="pdf/ICNSC2020Award.pdf"  class="bluelink">[Award]</a>
      <br>
      <div id="child2" style="display:none" class="bibtexdiv">
        @InProceedings{chen2020robot,<br>
          title = {Robot Navigation with Map-Based Deep Reinforcement Learning},<br>
          author = {Chen, Guangda and Pan, Lifan and Chen, Yu'an and Xu, Pei and Wang, Zhiqiang and Wu, Peichen and Ji, Jianmin and Chen, Xiaoping},<br>
          year = {2020},<br>
          pages = {1-6},<br>
          address = {Nanjing, China},<br>
          doi = {10.1109/ICNSC48988.2020.9238090},<br>
          organization = {IEEE}<br>
        }
      </div> 
      <div id="abs1" style="display:none" class="bibtexdiv">
        <b>Abstract:</b> This paper proposes an end-to-end deep reinforcement learning approach for mobile robot navigation with
        dynamic obstacles avoidance. Using experience collected in a
        simulation environment, a convolutional neural network (CNN)
        is trained to predict proper steering actions of a robot from its
        egocentric local occupancy maps, which accommodate various
        sensors and fusion algorithms. The trained neural network is
        then transferred and executed on a real-world mobile robot to
        guide its local path planning. The new approach is evaluated
        both qualitatively and quantitatively in simulation and realworld robot experiments. The results show that the map-based
        end-to-end navigation model is easy to be deployed to a robotic
        platform, robust to sensor noise and outperforms other existing
        DRL-based models in many indicators.
        <br>
        <b>Keywords:</b> robot navigation, obstacle avoidance, reinforcement learning, occupancy map
      </div> 
      <br>
    </div>
    <div class="width4 leftfloat" align="left">
      [4]
    </div>
    <div class="width96 linebigheight" align="left" id="ref2">
      <a href="https://ieeexplore.ieee.org/document/8588983"  class="fontnorm bluelink">Accurate Intrinsic and Extrinsic Calibration of RGB-D Cameras with GP-based Depth Correction</a>
      <br>
      <a class="pub-authors"><b>Guangda Chen</b>, Guowei Cui, Zhongxiao Jin, </a><a href="http://staff.ustc.edu.cn/~wufeng02/"  class="bluelink">Feng Wu</a><a class="pub-authors"> and Xiaoping Chen.</a>
      <br>
      <I><a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=7361"  class="bluelink pub-venue"><b>IEEE Sensors Journal</b></a></I> (Volume: 19 , <a href="https://ieeexplore.ieee.org/xpl/tocresult.jsp?isnumber=8662729"  class="bluelink">Issue: 7</a> , April, 1, 2019. <a href="http://www.greensci.net/search?kw=1530-437x"  class="bluelink">IF: 3.073</a>). 
      <br>
      <a id="main1" class="bluelink" onclick="document.all.child1.style.display=(document.all.child1.style.display =='none')?'':'none'" >
        [BibTeX]</a>, 
        <a class="bluelink" onclick="document.all.abs2.style.display=(document.all.abs2.style.display =='none')?'':'none'" >
          [Abstract]</a>, 
        <a href="pdf/rgbd_cal_j.pdf"  class="bluelink">[PDF]</a>, 
        <a href="pdf/jsen.pdf"  class="bluelink">[PDF_2]</a>
        <div id="child1" style="display:none" class="bibtexdiv">
          @Article{chen2019accurate,<br>
            title = {Accurate Intrinsic and Extrinsic Calibration of {RGB}-D Cameras With {GP}-Based Depth Correction},<br>
            author = {Guangda Chen and Guowei Cui and Zhongxiao Jin and Feng Wu and Xiaoping Chen},<br>
            journal = {IEEE Sensors Journal},<br>
            volume = {19},<br>
            number = {7},<br>
            pages = {2685--2694},<br>
            year = 2019,<br>
            month = {apr},<br>
            publisher = {IEEE},<br>
            doi = {10.1109/jsen.2018.2889805},<br>
            url = {https://doi.org/10.1109%2Fjsen.2018.2889805}<br>
          } 
        </div> 
        <div id="abs2" style="display:none" class="bibtexdiv">
          <b>Abstract:</b> In recent years, more and more robots have been
          equipped with low-cost RGB-D sensors, such as Microsoft Kinect
          and Intel Realsense, for safe navigation and active interaction
          with objects and people. In order to obtain more accurate
          and reliable fused color and depth information (coloured point
          clouds), not only the intrinsic and extrinsic parameters of color
          and depth sensor should be precisely calibrated, but also the
          external corrections of depth measurements are required. In
          this paper, using motion capture system, we propose a reliable
          calibration framework that enables the precise estimation of the
          intrinsic and extrinsic parameters of RGB-D sensors and provide
          a model-free depth calibration method based on heteroscedastic
          Gaussian Processes. Compared with the existing depth correction
          techniques, our method can simultaneously estimate the mean
          and variance of the depth error at different measurement
          distances, i.e., the probability distribution of the depth error
          relative to the measured distance, which is essential in the state
          estimation problems. To verify the effectiveness of our approach,
          we conduct a thorough qualitative and quantitative analysis of
          the major steps of our calibration method, and compare our
          experimental results with other related work. Furthermore, we
          demonstrate an experiment about the overall improvement of
          visual SLAM with a Kinect device calibrated by our calibration
          technique.
          <br>
          <b>Keywords:</b> RGB-D cameras, calibration, motion capture system
        </div> 
      <br>
      <br>
    </div>
  </div>
  <hr color=#987cb9 SIZE=1 />
</div>

<div id="project">
  <div class="width25 leftfloat" align="left" id="project_title">
    <br>
    <a class="fontbig">Project</a>
  </div>
  <div class="width75 leftfloat linebigheight" align="left" id="project_content">
    <p>
    <a href="http://www.robotics-kejia.com/about.php"  class="fontnorm bluelink"> <b>Kejia Robot</b></a>
    <br>
    Team: <a href="http://ai.ustc.edu.cn/en/robocup/atHome"  class="fontnorm bluelink">WrightEagle@Home</a>
    <br>
    Competitions:
    <br>
    - <a href="http://www.rcccaa.org/zdy/gui.html"  class="bluelink"> RoboCup China Open@Home league 2015 </a><a class="pub-authors fontsmall">(Guiyang, China)</a>
    <br>
    - <a href="https://www.robocup.org/events/5"  class="bluelink"> RoboCup@Home league 2016 </a><a class="pub-authors fontsmall">(Leipzig, Germany)</a>
    <br>
    - <a href="https://www.robocup.org/events/14"  class="bluelink"> Pre-RoboCup Asia-Pacific Competition </a><a class="pub-authors fontsmall">(Beijing, China)</a>
    <br>
    - <a href="https://www.robocup.org/events/6"  class="bluelink"> RoboCup@Home league 2017 </a><a class="pub-authors fontsmall">(Nagoya, Japan)</a>
    <br>
    - <a href="http://robo-tend.ustc.edu.cn/results.html"  class="bluelink"> The IJCAI-2019 Eldercare Robot Challenges </a><a class="pub-authors fontsmall">(Macau, China)</a>
    <br>
    &emsp;<a href="pdf/tdp19.pdf"  class="bluelink">[TDP <img src="./pic/pdf.gif" />]</a>, <a href="pdf/poster19_athome.pdf"  class="bluelink">[Poster <img src="./pic/pdf.gif" />]</a>, <a href="pdf/ECRDC_USTC.pdf"  class="bluelink">[Report <img src="./pic/pdf.gif" />]</a>, <a href="https://youtu.be/sWi9EOKIhlE"  class="bluelink">[Demo]</a>
    <br>
    <a class="fontsmall"><em>2015 — 2019</em></a>
    <br>
    </p>
    <p>
      <a href="https://optitrack.com/"  class="fontnorm bluelink"> <b>Motion Capture System</b></a>
      <br>
      for Testing: 
      <br>
      - <a href="pdf/Cleaning_Robots_Test.pdf" class="bluelink">Cleaning Robots Test</a><br>
      - <a href="http://roboticsbase.ustc.edu.cn/"  class="bluelink">Anhui Robot Technology Standard Innovation Base</a> 
      <br>
      for Calibration:
      <br>
      - <a href="http://staff.ustc.edu.cn/~wufeng02/doc/pdf/ZCWicira17.pdf"  class="bluelink">General Batch-Calibration Framework</a>
      <br>
      - <a href="#ref2" class="bluelink">RGB-D Cameras Calibration [ref.4]</a>
      <br>
      for Training:
      <br>
      - <a href="http://www.sohu.com/a/217691740_100071565"  class="bluelink">Real Simulation Unified Platform</a>
      <br>
      <a class="fontsmall"><em>2016 — 2018</em></a>
      <br>
    </p>
    <p>
      <a  class="fontnorm bluelink"> <b>DRL-based Navigation</b></a>
      <br>
      - <a href="#ref1" class="bluelink"> DQN-based Obstacle Avoidance [ref.3]</a>, <a href="https://youtu.be/Eq4AjsFH_cU"  class="bluelink">[Demo]</a>
      <br>
      - <a href="#ref0" class="bluelink"> Multi-Robot Collision Avoidance [ref.1, 2]</a>, <a href="https://youtu.be/KOb1q23L7-U"  class="bluelink">[Demo]</a>
      <br>
      <a class="fontsmall"><em>2019 — now</em></a>
      <br>
    </p>
  </div>
  <hr color=#987cb9 SIZE=1 />
</div>

<div id="service">
  <div class="width25 leftfloat" align="left" id="service_title">
    <br>
    <a class="fontbig">Other</a>
  </div>
  <div class="width75 leftfloat linebigheight" align="left" id="service_content">
    <p>
    Reviewer : 
    <a href="https://www.iros2019.org/"  class="fontnorm bluelink"> IROS 2019</a>, 
    <a href="http://www.icmeie.com/Default.aspx"  class="fontnorm bluelink"> MEIE 2019, 2020</a>, 
    <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=6287639"  class="fontnorm bluelink"> IEEE Access</a>, 
    <a href="http://www.csaeconf.org/"  class="fontnorm bluelink"> CSAE 2020</a>,
    <a href="https://www.hindawi.com/journals/complexity/"  class="fontnorm bluelink"> Complexity</a>,
    <a href="https://www.peertechz.com/journals/annals-of-robotics-and-automation"  class="fontnorm bluelink"> Annals of Robotics and Automation</a>,
    <a href="http://www.icra2021.org/"  class="fontnorm bluelink"> ICRA 2021</a>
    <br>
    </p>
    <p>
    Some links : 
    <a href="https://www.douban.com/doulist/46245195/"  class="fontnorm bluelink">Favorite movies</a>, 
    <a href="http://arxivdaily.com/topic/148"  class="fontnorm bluelink">arxivdaily.cs.RO</a>
    <br>
    </p>
  </div>
  <hr color=#987cb9 SIZE=2 />
</div>

<div>
  <a href="https://info.flagcounter.com/S8jX"><img src="https://s01.flagcounter.com/countxl/S8jX/bg_FFFFFF/txt_000000/border_CCCCCC/columns_2/maxflags_10/viewers_0/labels_1/pageviews_1/flags_0/percent_0/" alt="Flag Counter" border="0"></a>
  <div id="disclaimers" style="display:none" class="bibtexdiv">
    &emsp;The material provided on this website is to ensure the timely dissemination of academic and technical work. 
    Copyright and all rights therein are retained by authors or by other copyright holders. 
    All people copying this information are expected to adhere to the terms and constraints invoked by each author's copyright. 
    In most cases, these works may not be reposted without the explicit permission of the copyright holder.
  </div> 
  <div class="maxwidth" align="center" id="cpright">
    <p><a>&copy; </a>
    <a href="/" class="bluelink">Guangda Chen</a> 2019-2020. 
    <a onclick="document.all.disclaimers.style.display=(document.all.disclaimers.style.display =='none')?'':'none'" class="bluelink">Click to read the disclaimer</a></p>
  </div>
</div>
<!-- <script type="text/javascript" src="./main.js"></script> -->
<script>
  // 当网页向下滑动 20px 出现"返回顶部" 按钮
  window.onscroll = function() {scrollFunction()};
   
  function scrollFunction() {console.log(121);
      if (document.body.scrollTop > 20 || document.documentElement.scrollTop > 20) {
          document.getElementById("myBtn").style.display = "block";
      } else {
          document.getElementById("myBtn").style.display = "none";
      }
  }
   
  // 点击按钮，返回顶部
  function topFunction() {
      document.body.scrollTop = 0;
      document.documentElement.scrollTop = 0;
  }
  </script>
<script type="text/javascript">
function get_width()
{
  screen_w = window.screen.availWidth;
  if (screen_w > 1000)
  {
    return 1000;
  }
  else
  {
    return screen_w;
  }
}
document.getElementById("page_size").style = "width:"+get_width() +"px";
if (get_width() < 1000) {

  maxline = "maxwidth leftfloat";
  document.getElementById("title_div_left").className = maxline;
  document.getElementById("title_div_right").className = maxline;
  // document.getElementById("imglove").style = "";
  document.getElementById("title_div_right").align = "left";
  document.getElementById("about_title").className = maxline;
  document.getElementById("about_content").className = maxline;
  document.getElementById("edu_title").className = maxline;
  document.getElementById("edu_content").className = maxline;
  document.getElementById("project_title").className = maxline;
  document.getElementById("project_content").className = maxline;
  document.getElementById("pub_title").className = maxline;
  document.getElementById("pub_content").className = maxline;
  document.getElementById("service_title").className = maxline;
  document.getElementById("service_content").className = maxline;
  
  var e = document.getElementById("about_title");
  e.innerHTML = e.innerHTML.replace('<br>', '');
  // e = document.getElementById("title_br");
  // e.innerHTML = e.innerHTML.replace('<br>', '');
  var e = document.getElementById("img_br");
  e.innerHTML = e.innerHTML.replace('<br><br><br>', '');
  var e = document.getElementById("edu_title");
  e.innerHTML = e.innerHTML.replace('<br>', '');
  var e = document.getElementById("project_title");
  e.innerHTML = e.innerHTML.replace('<br>', '');
  var e = document.getElementById("pub_title");
  e.innerHTML = e.innerHTML.replace('<br>', '');
  var e = document.getElementById("service_title");
  e.innerHTML = e.innerHTML.replace('<br>', '');
}
</script>
</div>
</div>
</div>
</body>
</html>
